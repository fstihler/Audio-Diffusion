{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ab43b8f-d658-475e-8158-b266e826af82",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Diffusion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e005302f-510c-48da-8f35-d4621e539d9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 1536)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wav to melspectrum. # TODO: out of the box code took like 10s\n",
    "import librosa\n",
    "import numpy as np\n",
    "y, sr = librosa.load('test.wav', sr=None)\n",
    "S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "mel_spec = librosa.power_to_db(S, ref=np.max)\n",
    "mel_spec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa1906d9-9011-444a-b4b0-cf8f2408bd7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 1088)\n"
     ]
    }
   ],
   "source": [
    "# custom wav2mels\n",
    "import torchvision\n",
    "\n",
    "class MelSpectrogram(object):\n",
    "    def __init__(self, sr, nfft, fmin, fmax, nmels, hoplen, spec_power, inverse=False):\n",
    "        self.sr = sr\n",
    "        self.nfft = nfft\n",
    "        self.fmin = fmin\n",
    "        self.fmax = fmax\n",
    "        self.nmels = nmels\n",
    "        self.hoplen = hoplen\n",
    "        self.spec_power = spec_power\n",
    "        self.inverse = inverse\n",
    "\n",
    "        self.mel_basis = librosa.filters.mel(sr=sr, n_fft=nfft, fmin=fmin, fmax=fmax, n_mels=nmels)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.inverse:\n",
    "            spec = librosa.feature.inverse.mel_to_stft(\n",
    "                x, sr=self.sr, n_fft=self.nfft, fmin=self.fmin, fmax=self.fmax, power=self.spec_power\n",
    "            )\n",
    "            wav = librosa.griffinlim(spec, hop_length=self.hoplen)\n",
    "            return wav\n",
    "        else:\n",
    "            spec = np.abs(librosa.stft(x, n_fft=self.nfft, hop_length=self.hoplen)) ** self.spec_power\n",
    "            mel_spec = np.dot(self.mel_basis, spec)\n",
    "            return mel_spec\n",
    "        \n",
    "TRANSFORMS = torchvision.transforms.Compose([\n",
    "    MelSpectrogram(sr=sr, nfft=1024, fmin=125, fmax=7600, nmels=128, hoplen=250, spec_power=1),\n",
    "    # LowerThresh(1e-5),\n",
    "    # Log10(),\n",
    "    # Multiply(20),\n",
    "    # Subtract(20),\n",
    "    # Add(100),\n",
    "    # Divide(100),\n",
    "    # Clip(0, 1.0),\n",
    "    #TrimSpec(trim_len)\n",
    "])\n",
    "\n",
    "l = int(librosa.get_duration(path='./test.wav'))\n",
    "sr = 16000\n",
    "time = l\n",
    "length = sr * time \n",
    "y = y.reshape(-1)\n",
    "# print(sr)\n",
    "# this cannot be a transform without creating a huge overhead with inserting audio_name in each\n",
    "y = np.zeros(length)\n",
    "if y.shape[0] < length:\n",
    "    y[:len(y)] = y\n",
    "else:\n",
    "    y = y[:length]\n",
    "\n",
    "# # wav:\n",
    "y = y[ : length - 1]        # ensure: 640 spec\n",
    "\n",
    "mel_spec = TRANSFORMS(y)\n",
    "print(mel_spec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88675ba-ec83-439e-b54d-fa3b8b309682",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# encode melspectrum to latents with frozen VAE\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, UNet2DConditionModel\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "              \"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\"\n",
    ")\n",
    "# Freeze vae\n",
    "vae.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65db3d51-aab8-46b8-9a85-f707b8599d89",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Video Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0fd5dbc-8211-46bc-9e0e-f6e887d475b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from preprocessing.preprocessing import VideoFrameDataset, ImglistToTensor\n",
    "from torchvision import transforms\n",
    "import os\n",
    "\n",
    "videos_root = os.path.join('data/videos_root')\n",
    "annotation_file = os.path.join(videos_root, 'annotations.txt')\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "        ImglistToTensor(),  # list of PIL images to (FRAMES x CHANNELS x HEIGHT x WIDTH) tensor\n",
    "        # transforms.Resize(299),  # image batch, resize smaller edge to 299\n",
    "        # transforms.CenterCrop(299),  # image batch, center crop to square 299x299\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "dataset = VideoFrameDataset(\n",
    "        root_path=videos_root,\n",
    "        annotationfile_path=annotation_file,\n",
    "        num_segments=54,\n",
    "        frames_per_segment=1,\n",
    "        imagefile_template='{:06d}.jpg',\n",
    "        transform=preprocess,\n",
    "        test_mode=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b11cb08-3799-432c-8f1d-cbc6fd591b75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x,idx = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c2ebb2f-ad09-4f6b-9037-d06f2348a564",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting av\n",
      "  Downloading av-12.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m34.7/34.7 MB\u001B[0m \u001B[31m35.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: av\n",
      "\u001B[33m  WARNING: The script pyav is installed in '/accounts/class/s222/deepul_vidaud/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001B[0m\u001B[33m\n",
      "\u001B[0mSuccessfully installed av-12.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f1e8bfe-d328-4cdb-b3a1-6cb74f18a03f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 1568, 768]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import av\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoImageProcessor, VideoMAEModel\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    '''\n",
    "    Sample a given number of frame indices from the video.\n",
    "    Args:\n",
    "        clip_len (`int`): Total number of frames to sample.\n",
    "        frame_sample_rate (`int`): Sample every n-th frame.\n",
    "        seg_len (`int`): Maximum allowed index of sample's last frame.\n",
    "    Returns:\n",
    "        indices (`List[int]`): List of sampled frame indices\n",
    "    '''\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "    start_idx = end_idx - converted_len\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices\n",
    "\n",
    "\n",
    "# video clip consists of 300 frames (10 seconds at 30 FPS)\n",
    "file_path = hf_hub_download(\n",
    "    repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n",
    ")\n",
    "container = av.open(file_path)\n",
    "\n",
    "# sample 16 frames\n",
    "indices = sample_frame_indices(clip_len=16, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n",
    "video = read_video_pyav(container, indices)\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "\n",
    "# prepare video for the model\n",
    "inputs = image_processor(list(video), return_tensors=\"pt\")\n",
    "\n",
    "# forward pass\n",
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "list(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a175478-ec1b-4946-a8af-ffcac6d9951b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 3, 224, 224])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50906913-f1bf-4d92-bf19-105694ebf3aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoMAEModel(\n",
       "  (embeddings): VideoMAEEmbeddings(\n",
       "    (patch_embeddings): VideoMAEPatchEmbeddings(\n",
       "      (projection): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
       "    )\n",
       "  )\n",
       "  (encoder): VideoMAEEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x VideoMAELayer(\n",
       "        (attention): VideoMAEAttention(\n",
       "          (attention): VideoMAESelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VideoMAESelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VideoMAEIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): VideoMAEOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5b7fe63b-14d5-495c-85ed-6b0675f97d3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, VideoMAEForPreTraining\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "num_frames = 16\n",
    "model = VideoMAEForPreTraining.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "num_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\n",
    "seq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\n",
    "bool_masked_pos = torch.randint(size=(1, seq_length)).bool()\n",
    "\n",
    "outputs = model(**inputs, bool_masked_pos=bool_masked_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a4eb96de-8f7a-4ec6-a576-7564beb948b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1568, 1536])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84fc169e-fd39-4a2d-ad14-9d9bd47e6ee5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 3, 224, 224])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c081d6a-4919-48e1-affe-7c2ff7416c2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4182000/1538938368.py:2: UserWarning: Using a target size (torch.Size([1, 776, 768])) that is different to the input size (torch.Size([1, 16, 3, 224, 224])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  F.mse_loss(inputs.pixel_values,outputs.last_hidden_state,reduce='mean')\n",
      "/usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (224) must match the size of tensor b (768) at non-singleton dimension 4",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[27], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m functional \u001B[38;5;28;01mas\u001B[39;00m F\n\u001B[0;32m----> 2\u001B[0m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmse_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpixel_values\u001B[49m\u001B[43m,\u001B[49m\u001B[43moutputs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlast_hidden_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43mreduce\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmean\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages/torch/nn/functional.py:3294\u001B[0m, in \u001B[0;36mmse_loss\u001B[0;34m(input, target, size_average, reduce, reduction)\u001B[0m\n\u001B[1;32m   3291\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3292\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[0;32m-> 3294\u001B[0m expanded_input, expanded_target \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbroadcast_tensors\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3295\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_nn\u001B[38;5;241m.\u001B[39mmse_loss(expanded_input, expanded_target, _Reduction\u001B[38;5;241m.\u001B[39mget_enum(reduction))\n",
      "File \u001B[0;32m/usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages/torch/functional.py:74\u001B[0m, in \u001B[0;36mbroadcast_tensors\u001B[0;34m(*tensors)\u001B[0m\n\u001B[1;32m     72\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function(tensors):\n\u001B[1;32m     73\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(broadcast_tensors, tensors, \u001B[38;5;241m*\u001B[39mtensors)\n\u001B[0;32m---> 74\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbroadcast_tensors\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: The size of tensor a (224) must match the size of tensor b (768) at non-singleton dimension 4"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "F.mse_loss(inputs.pixel_values,outputs.last_hidden_state,reduce='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff77e6a-c9b8-41b4-942a-044aadb499c8",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Latent diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a36d7a-c7a3-4c74-bf60-40ab93031795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent diffusion \n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    args.pretrained_model_name_or_path, subfolder=\"unet\", revision=args.non_ema_revision\n",
    ")\n",
    "unet.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "936ac342-0944-42c5-9ec4-0651b70fdb4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train unet\n",
    "progress_bar = tqdm(\n",
    "        range(0, args.max_train_steps),\n",
    "        initial=initial_global_step,\n",
    "        desc=\"Steps\",\n",
    "        # Only show the progress bar once on each machine.\n",
    "        disable=not accelerator.is_local_main_process,\n",
    "    )\n",
    "\n",
    "for epoch in range(first_epoch, args.num_train_epochs):\n",
    "    train_loss = 0.0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        with accelerator.accumulate(unet):\n",
    "            # Convert images to latent space\n",
    "            latents = vae.encode(batch[\"pixel_values\"].to(weight_dtype)).latent_dist.sample()\n",
    "            latents = latents * vae.config.scaling_factor\n",
    "\n",
    "            # Sample noise that we'll add to the latents\n",
    "            noise = torch.randn_like(latents)\n",
    "            if args.noise_offset:\n",
    "                # https://www.crosslabs.org//blog/diffusion-with-offset-noise\n",
    "                noise += args.noise_offset * torch.randn(\n",
    "                    (latents.shape[0], latents.shape[1], 1, 1), device=latents.device\n",
    "                )\n",
    "            if args.input_perturbation:\n",
    "                new_noise = noise + args.input_perturbation * torch.randn_like(noise)\n",
    "            bsz = latents.shape[0]\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
    "            timesteps = timesteps.long()\n",
    "\n",
    "            # Add noise to the latents according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process)\n",
    "            if args.input_perturbation:\n",
    "                noisy_latents = noise_scheduler.add_noise(latents, new_noise, timesteps)\n",
    "            else:\n",
    "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            # Get the text embedding for conditioning\n",
    "            encoder_hidden_states = text_encoder(batch[\"input_ids\"], return_dict=False)[0]\n",
    "\n",
    "            # Get the target for loss depending on the prediction type\n",
    "            if args.prediction_type is not None:\n",
    "                # set prediction_type of scheduler if defined\n",
    "                noise_scheduler.register_to_config(prediction_type=args.prediction_type)\n",
    "\n",
    "            if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "                target = noise\n",
    "            elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "                target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
    "\n",
    "            # Predict the noise residual and compute loss\n",
    "            model_pred = unet(noisy_latents, timesteps, encoder_hidden_states, return_dict=False)[0]\n",
    "\n",
    "            if args.snr_gamma is None:\n",
    "                loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "            else:\n",
    "                # Compute loss-weights as per Section 3.4 of https://arxiv.org/abs/2303.09556.\n",
    "                # Since we predict the noise instead of x_0, the original formulation is slightly changed.\n",
    "                # This is discussed in Section 4.2 of the same paper.\n",
    "                snr = compute_snr(noise_scheduler, timesteps)\n",
    "                mse_loss_weights = torch.stack([snr, args.snr_gamma * torch.ones_like(timesteps)], dim=1).min(\n",
    "                    dim=1\n",
    "                )[0]\n",
    "                if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "                    mse_loss_weights = mse_loss_weights / snr\n",
    "                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "                    mse_loss_weights = mse_loss_weights / (snr + 1)\n",
    "\n",
    "                loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"none\")\n",
    "                loss = loss.mean(dim=list(range(1, len(loss.shape)))) * mse_loss_weights\n",
    "                loss = loss.mean()\n",
    "\n",
    "            # Gather the losses across all processes for logging (if we use distributed training).\n",
    "            avg_loss = accelerator.gather(loss.repeat(args.train_batch_size)).mean()\n",
    "            train_loss += avg_loss.item() / args.gradient_accumulation_steps\n",
    "\n",
    "            # Backpropagate\n",
    "            accelerator.backward(loss)\n",
    "            if accelerator.sync_gradients:\n",
    "                accelerator.clip_grad_norm_(unet.parameters(), args.max_grad_norm)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa9a29c-030c-4af1-ad0a-63e6c26c56d3",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15766bf5-d0f2-4b7e-8fde-056a76a7b695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, video):\n",
    "    \"\"\"Utility to run inference given a model and test video.\n",
    "    \n",
    "    The video is assumed to be preprocessed already.\n",
    "    \"\"\"\n",
    "    # (num_frames, num_channels, height, width)\n",
    "    perumuted_sample_test_video = video.permute(1, 0, 2, 3)\n",
    "\n",
    "    inputs = {\n",
    "        \"pixel_values\": perumuted_sample_test_video.unsqueeze(0),\n",
    "        \"labels\": torch.tensor(\n",
    "            [sample_test_video[\"label\"]]\n",
    "        ),  # this can be skipped if you don't have labels available.\n",
    "    }\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    model = model.to(device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414d1ba3-cf0d-447f-bb49-18660738f7bd",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Extra: out-of-the-box Diffusion pipeline\n",
    "This is only for illustration as finetunning is not recomended with the pipeline\n",
    "1. `pip install --upgrade diffusers transformers scipy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56c3d970-84db-428c-9587-b0406838521d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: diffusers in /accounts/class/s222/deepul_vidaud/.local/lib/python3.11/site-packages (0.27.2)\n",
      "Requirement already satisfied: transformers in /accounts/class/s222/deepul_vidaud/.local/lib/python3.11/site-packages (4.39.3)\n",
      "Requirement already satisfied: scipy in /accounts/class/s222/deepul_vidaud/.local/lib/python3.11/site-packages (1.13.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from diffusers) (5.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from diffusers) (3.8.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.2 in /accounts/class/s222/deepul_vidaud/.local/lib/python3.11/site-packages (from diffusers) (0.22.2)\n",
      "Requirement already satisfied: numpy in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from diffusers) (1.23.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from diffusers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from diffusers) (2.28.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /accounts/class/s222/deepul_vidaud/.local/lib/python3.11/site-packages (from diffusers) (0.4.2)\n",
      "Requirement already satisfied: Pillow in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from diffusers) (9.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /accounts/class/s222/deepul_vidaud/.local/lib/python3.11/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /accounts/class/s222/deepul_vidaud/.local/lib/python3.11/site-packages (from huggingface-hub>=0.20.2->diffusers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from huggingface-hub>=0.20.2->diffusers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from importlib-metadata->diffusers) (3.11.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from requests->diffusers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from requests->diffusers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from requests->diffusers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from requests->diffusers) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade diffusers transformers scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7617c6f3-8fd6-4e82-92e1-436fd858c31f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: accelerate in /accounts/class/s222/deepul_vidaud/.local/lib/python3.11/site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from accelerate) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: psutil in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from accelerate) (2.0.0)\n",
      "Requirement already satisfied: huggingface-hub in /accounts/class/s222/deepul_vidaud/.local/lib/python3.11/site-packages (from accelerate) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /accounts/class/s222/deepul_vidaud/.local/lib/python3.11/site-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from packaging>=20.0->accelerate) (3.0.9)\n",
      "Requirement already satisfied: filelock in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.8.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /accounts/class/s222/deepul_vidaud/.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /accounts/class/s222/deepul_vidaud/.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (65.5.1)\n",
      "Requirement already satisfied: wheel in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (0.38.4)\n",
      "Requirement already satisfied: cmake in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.26.1)\n",
      "Requirement already satisfied: lit in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /accounts/class/s222/deepul_vidaud/.local/lib/python3.11/site-packages (from huggingface-hub->accelerate) (2024.3.1)\n",
      "Requirement already satisfied: requests in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from huggingface-hub->accelerate) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from huggingface-hub->accelerate) (4.64.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "981b9e9d-09f3-41fd-8f6a-e20ac61980e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/accounts/class/s222/deepul_vidaud/.local/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2024-04-08 14:47:25.659022: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-08 14:47:27.352847: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-08 14:47:34.436275: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f0d289fd1a428a95eb55dec3894018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_index.json:   0%|          | 0.00/541 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b34b39df60c944b09b4362c7a675f4e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7353a88f0ad14c908433d5d8215ac8b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder/model.safetensors:   0%|          | 0.00/492M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "071f8b92fb364202a6b42e857c491ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)kpoints/scheduler_config-checkpoint.json:   0%|          | 0.00/209 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "140e67a463944eebada0e55cdb586547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "safety_checker/config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e86090c586824b61a43b61d20f6b2400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder/config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e9a4ca9d74345358bfe48e43c69b7fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "safety_checker/model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3bc555bbdd6466796a4e5f0b8eedd41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ature_extractor/preprocessor_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04d6a95c81a34160bf12895943709082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "119bad6c2b574597af73e838d759f028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler/scheduler_config.json:   0%|          | 0.00/313 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31281062365d4b51a08abe04b4d05303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/tokenizer_config.json:   0%|          | 0.00/806 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae198993849644888455f249ab1d64f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0ae2f07d3d4445a98d4897f51afe0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/special_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b0393e909d46cdb65bee37d08429a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unet/config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "424daa7aa02145caaa96b2ceb2287522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unet/diffusion_pytorch_model.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993e7af45e4444b1af33742518bb53e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vae/diffusion_pytorch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90361c8db8e242b0bc0d704ebe1aa1a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vae/config.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ba1a2ea19fb4952865cbb7d53be2009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02908a89a8f446c89e2cabf93917f648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# stablediffusion pipeline\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "device = \"cuda\"\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "image = pipe(prompt).images[0]  \n",
    "image.save(\"astronaut_rides_horse.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0902c35-308b-4323-85e3-e1b40217e0d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
